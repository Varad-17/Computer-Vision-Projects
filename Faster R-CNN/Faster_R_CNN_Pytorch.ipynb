{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61e1fcc6"
      },
      "source": [
        "# Faster R-CNN\n",
        "\n",
        "In this notebook we will try to implement faster R-CNN from scratch in pytorch. We will divide this process into two steps - \n",
        "1. Building the Region Proposal Network \n",
        "2. Building the Classifier to classify the object"
      ],
      "id": "61e1fcc6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPkwbUX78et_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "jPkwbUX78et_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEQZIeI38Pb1"
      },
      "outputs": [],
      "source": [
        "! unzip \"drive/MyDrive/images.zip\""
      ],
      "id": "JEQZIeI38Pb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d5fa4d9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let us get the test image to visualize the Anchor boxes\n",
        "\n",
        "test_img = cv2.imread(\"drive/MyDrive/spidey.jpg\")\n",
        "test_img = cv2.resize(test_img,(800,800))\n",
        "cv2.imwrite(\"meme.jpg\",test_img)\n",
        "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "print(\"Our image is\")\n",
        "plt.imshow(test_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "id": "1d5fa4d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bc5369f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "labels = pd.read_csv(\"drive/MyDrive/spidey_labels.csv\")\n",
        "labels['x1'] = labels['bbox_x']\n",
        "labels['x2'] = labels['bbox_x'] + labels['bbox_width']\n",
        "labels['y1'] = labels['bbox_y']\n",
        "labels['y2'] = labels['bbox_y'] + labels['bbox_height']\n",
        "\n",
        "new_df = labels[['y1','x1','y2','x2']]\n",
        "\n",
        "clone = test_img.copy()\n",
        "\n",
        "for s in new_df.values :\n",
        "    y1,x1,y2,x2 = s\n",
        "    cv2.rectangle(clone, (x1,y1), (x2,y2), (255,255,255), 2)\n",
        "\n",
        "    \n",
        "plt.imshow(clone)\n",
        "plt.show()"
      ],
      "id": "7bc5369f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e853f95f"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg16\n",
        "\n",
        "model = vgg16(weights = 'DEFAULT')\n",
        "fe = list(model.features)\n",
        "print(\"This is the list of all the layers that VGG16 contains\")\n",
        "print(fe)\n",
        "\n",
        "# NOW WE NEED TO SEE HOW MANY LAYERS OF VGG TO USE\n",
        "# FOR THIS PURPOSE, WE WILL PASS A DUMMY IMAGE UNTIL IT IS \n",
        "# REDUCED TO 1/16 OF ITS ORIGINAL SIZE\n",
        "\n",
        "k = torch.zeros((1, 3, 800, 800)).float()\n",
        "req_features = []\n",
        "for i in fe:\n",
        "    k = i(k)\n",
        "    if k.size()[2] < 800//8:\n",
        "        break\n",
        "    req_features.append(i)\n",
        "    out_channels = k.size()[1]\n",
        "    \n",
        "print()\n",
        "print()\n",
        "print(\"The number of features of VGG16 we require and number of output channels are \")\n",
        "print(len(req_features)) #30\n",
        "print(out_channels) # 512"
      ],
      "id": "e853f95f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ef1cb0e"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# Let us now make the backbone of the RPN \n",
        "\n",
        "test_img1 = test_img.copy()\n",
        "fe_extractor = nn.Sequential(*req_features)\n",
        "\n",
        "test_img1 = torch.tensor(test_img1, dtype = torch.float)\n",
        "test_img1 = torch.permute(torch.unsqueeze(test_img1, dim = 0),(0,3,1,2))\n",
        "feature_map = fe_extractor(test_img1)\n",
        "print(feature_map.shape)"
      ],
      "id": "3ef1cb0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f690ef7b"
      },
      "outputs": [],
      "source": [
        "# Now we need to create anchor boxes for all the \n",
        "# anchor centres in the feature map\n",
        "\n",
        "# First let us visualize all the anchor centres in the image\n",
        "\n",
        "test_img2 = test_img.copy()\n",
        "centre_x = np.arange(4,800,8)\n",
        "centre_y = np.arange(4,800,8)\n",
        "\n",
        "for i in range(len(centre_x)):\n",
        "    for j in range(len(centre_y)):\n",
        "        cv2.circle(test_img2, (centre_x[i],centre_y[j]), 2, (255,255,255), -1)\n",
        "    \n",
        "plt.imshow(test_img2)\n",
        "plt.show()"
      ],
      "id": "f690ef7b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d077cb1"
      },
      "outputs": [],
      "source": [
        "# Time to create anchor boxes \n",
        "# But first we need to define scales and aspect ratio for boxes\n",
        "\n",
        "anchor_scales = [8,16, 32]\n",
        "aspect_ratios = [0.5, 1 ,2]\n",
        "\n",
        "# Now we need to create anchor boxes \n",
        "\n",
        "anchor_boxes = np.zeros((len(centre_x)*len(centre_y),\n",
        "                         len(anchor_scales)*len(aspect_ratios), 4),\n",
        "                        dtype = np.int32)\n",
        "\n",
        "for p in range(len(centre_x)*len(centre_y)):\n",
        "    for index in range(len(anchor_scales)*len(aspect_ratios)):\n",
        "            index_x = p // len(centre_x)\n",
        "            index_y = p % len(centre_x)\n",
        "            \n",
        "            q  = index // len(anchor_scales)\n",
        "            r  = index % len(aspect_ratios)\n",
        "            \n",
        "            h  = 8 * anchor_scales[q] * np.sqrt(aspect_ratios[r])\n",
        "            w  = 8 * anchor_scales[q] * np.sqrt(1/aspect_ratios[r])\n",
        "            \n",
        "            anchor_boxes[p, index, 0] = int(centre_x[index_x] - w / 2)\n",
        "            anchor_boxes[p, index, 1] = int(centre_y[index_y] - h / 2)\n",
        "            anchor_boxes[p, index, 2] = int(centre_x[index_x] + w / 2)\n",
        "            anchor_boxes[p, index, 3] = int(centre_y[index_y] + h / 2)\n",
        "            "
      ],
      "id": "3d077cb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46e527d7"
      },
      "outputs": [],
      "source": [
        "print(anchor_boxes.shape)\n",
        "\n",
        "# Here -ve sign indicates that the anchor box went out of the picture\n",
        "# Let us visualize the anchor boxes for the centre of the image\n",
        "\n",
        "index1 = 100*50 + 50\n",
        "boxes = anchor_boxes[index1]\n",
        "\n",
        "test_img3 = test_img.copy()\n",
        "\n",
        "for a,b,c,d in boxes :\n",
        "    cv2.rectangle(test_img3,(a,b),(c,d),(255,255,255),2)\n",
        "    \n",
        "plt.imshow(test_img3)\n",
        "plt.show()"
      ],
      "id": "46e527d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e399c05"
      },
      "outputs": [],
      "source": [
        "#First we need to find the valid anchor boxes\n",
        "\n",
        "condition = np.logical_and(np.all(anchor_boxes >= 0,axis = 2) , np.all(anchor_boxes <= 800,axis = 2 ))\n",
        "valid_boxes = anchor_boxes[condition]\n",
        "\n",
        "# We need to get the indices of the valid boxes as well\n",
        "indices = np.transpose(np.where(condition))\n"
      ],
      "id": "9e399c05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e9835c9"
      },
      "outputs": [],
      "source": [
        "def iou(boxA, boxB):\n",
        "       \n",
        "   # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    \n",
        "   # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    \n",
        "   # compute the area of both the prediction and ground-truth\n",
        "   # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] ) * (boxA[3] - boxA[1] )\n",
        "    boxBArea = (boxB[2] - boxB[0] ) * (boxB[3] - boxB[1] )\n",
        "   \n",
        "   # compute the intersection over union by taking the intersection\n",
        "   # area and dividing it by the sum of prediction + ground-truth\n",
        "   # areas - the intersection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "   \n",
        "    assert iou >= 0.0\n",
        "    assert iou <= 1.0\n",
        "    return iou"
      ],
      "id": "0e9835c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57ee0e99"
      },
      "outputs": [],
      "source": [
        "# Now we will start working on a dataset instead of a test image\n",
        "import os \n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "#path = \"/home/web_slinger/Documents/Machine Learning Projects/Object Detection Algorithms/Faster R-CNN\"\n",
        "path = \"drive/MyDrive\"\n",
        "gt_boxes = []\n",
        "raccoon_images = []\n",
        "annotations = pd.read_csv(os.path.join(path, \"raccoon_labels.csv\"))\n",
        "\n",
        "\n",
        "for image in os.listdir(\"images\"):\n",
        "    gt_box = []\n",
        "    data = cv2.imread(os.path.join(\"images\", image))\n",
        "    x_scale = 800 / data.shape[0]\n",
        "    y_scale = 800 / data.shape[1]\n",
        "    data = cv2.resize(data, (800,800))\n",
        "    raccoon_images.append(data)\n",
        "    label_df = annotations[annotations[\"filename\"] == image]\n",
        "    gt_box = (label_df.values)[:,-4:]\n",
        "    gt_box[:,0::2] = gt_box[:,0::2] * y_scale\n",
        "    gt_box[:,1::2] = gt_box[:,1::2] * x_scale \n",
        "    gt_boxes.append(gt_box.astype(int))"
      ],
      "id": "57ee0e99"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "375c7d2d"
      },
      "outputs": [],
      "source": [
        "# Let us visualize one of the images and the bounding box\n",
        "\n",
        "visualize = cv2.cvtColor(raccoon_images[0], cv2.COLOR_BGR2RGB)\n",
        "cv2.rectangle(visualize,(gt_boxes[0][0,0],gt_boxes[0][0,1]),(gt_boxes[0][0,2],gt_boxes[0][0,3]),(0,255,0),2)\n",
        "plt.imshow(visualize)\n",
        "plt.show()"
      ],
      "id": "375c7d2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd107b67"
      },
      "outputs": [],
      "source": [
        "# Now let us create an array containing labels for all the\n",
        "# valid anchor boxes\n",
        "\n",
        "labels = np.full((200,valid_boxes.shape[0]), -1)\n",
        "# Now we will find the anchor boxes which have iou scores \n",
        "# greater than postive threshold or iou scores less than \n",
        "# negative threshold\n",
        "\n",
        "pos_threshold = 0.7\n",
        "neg_threshold = 0.3\n",
        "max_ious = np.empty((200, valid_boxes.shape[0],4))\n",
        "\n",
        "for gt_index in range(len(gt_boxes)):\n",
        "    gt_values = gt_boxes[gt_index]\n",
        "    for i in range(len(valid_boxes)):\n",
        "        box =  valid_boxes[i]\n",
        "        max_iou = 0\n",
        "        max_index = 0\n",
        "        for j in range(len(gt_values)) :\n",
        "            gt = gt_values[j]\n",
        "            IoU = iou(gt, box)\n",
        "            if IoU > max_iou :\n",
        "                max_iou = IoU\n",
        "                max_index = j\n",
        "        max_ious[gt_index, i] = gt_values[max_index]\n",
        "        if max_iou >= pos_threshold :\n",
        "            labels[gt_index, i] = 1\n",
        "            \n",
        "        elif max_iou < neg_threshold :\n",
        "            labels[gt_index, i] = 0"
      ],
      "id": "fd107b67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1181fefd"
      },
      "outputs": [],
      "source": [
        "# we will find the highest iou for each gt_box \n",
        "# and its corresponding anchor box so that we will be able \n",
        "# to assign postive label for that box\n",
        "for gt_index in range(len(gt_boxes)) :\n",
        "    gt_values = gt_boxes[gt_index]\n",
        "    for gt in gt_values :\n",
        "        max_iou = 0\n",
        "        max_index = 0\n",
        "        for i in range(len(valid_boxes)) :\n",
        "            box = valid_boxes[i]\n",
        "            IoU = iou(gt, box)\n",
        "            if IoU > max_iou :\n",
        "                max_iou = IoU\n",
        "                max_index = i\n",
        "        labels[gt_index,max_index] = 1"
      ],
      "id": "1181fefd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daee21ff"
      },
      "outputs": [],
      "source": [
        "# Now we will need 100 total samples to feed the rpn from each image\n",
        "\n",
        "# We will divide the samples by taking 50 positive samples\n",
        "# and 50 negative samples\n",
        "\n",
        "n_pos = 50\n",
        "for i in range(len(labels)):\n",
        "    pos_index = np.where(labels[i] == 1)[0]\n",
        "    if len(pos_index) > n_pos :\n",
        "        disable_index = np.random.choice(pos_index,\n",
        "                                         size = (len(pos_index) - n_pos),replace = False)\n",
        "        labels[i][disable_index] = -1\n",
        "        \n",
        "    n_neg = 1000 - np.sum(labels[i] == 1)\n",
        "    neg_index = np.where(labels[i] == 0)[0]\n",
        "    \n",
        "    if len(neg_index) > n_neg:\n",
        "        disable_index = np.random.choice(neg_index, \n",
        "                                         size = (len(neg_index) - n_neg), replace = False)\n",
        "        labels[i][disable_index] = -1"
      ],
      "id": "daee21ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d7ac276"
      },
      "outputs": [],
      "source": [
        "# Now we need to parametrize to find the position of the \n",
        "# anchor box with respect to the ground truth boxes\n",
        "\n",
        "height = valid_boxes[:, 2] - valid_boxes[:, 0]\n",
        "width = valid_boxes[:, 3] - valid_boxes[:, 1]\n",
        "ctr_y = valid_boxes[:, 0] + 0.5 * height\n",
        "ctr_x = valid_boxes[:, 1] + 0.5 * width\n",
        "\n",
        "final_anchors = np.empty((200,valid_boxes.shape[0], 4))\n",
        "for i in range(200):\n",
        "    max_iou_bbox = max_ious[i]\n",
        "    base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
        "    base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
        "    base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
        "    base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
        "    \n",
        "    dy = (base_ctr_y - ctr_y) / height\n",
        "    dx = (base_ctr_x - ctr_x) / width\n",
        "    dh = np.log(base_height / height)\n",
        "    dw = np.log(base_width / width) \n",
        "    anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "    final_anchors[i] = anchor_locs"
      ],
      "id": "4d7ac276"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e2b2691"
      },
      "outputs": [],
      "source": [
        "anchor_labels = np.empty((200,90000), dtype = labels.dtype)\n",
        "anchor_labels.fill(-1)\n",
        "valid_indices = [p[0]*9 + p[1] for p in indices]\n",
        "anchor_labels[:,valid_indices] = labels"
      ],
      "id": "8e2b2691"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "383bc9b9"
      },
      "outputs": [],
      "source": [
        "anchor_locations = np.empty((200,90000,4))\n",
        "anchor_locations.fill(0)\n",
        "anchor_locations[:,valid_indices] = final_anchors"
      ],
      "id": "383bc9b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3e0a993"
      },
      "outputs": [],
      "source": [
        "# we need to make custom loss function for the bounding box offsets and objectness scores\n",
        "\n",
        "from torch.nn.functional import smooth_l1_loss, binary_cross_entropy\n",
        "\n",
        "def custom_l1_loss(pred, target, target_label):\n",
        "    assert (len(pred) == 90000 and len(pred[0]) == 4)\n",
        "    indices = (target_label == 1).nonzero()\n",
        "    valid_preds = pred[indices]\n",
        "    target_locs =  target[indices]\n",
        "    l1_loss = smooth_l1_loss(valid_preds, target_locs)\n",
        "    return torch.mean(l1_loss)\n",
        "\n",
        "def binary_loss(preds, target_labels):\n",
        "    assert len(preds) == 90000\n",
        "    indices = (target_labels != -1).nonzero()\n",
        "    predictions = preds[indices]\n",
        "    target = target_labels[indices]\n",
        "    bce_loss = binary_cross_entropy(predictions, target)\n",
        "    return bce_loss"
      ],
      "id": "e3e0a993"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94baa7d7"
      },
      "outputs": [],
      "source": [
        "# Time to make the model RPN\n",
        "\n",
        "class rpn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.reg_head = nn.Conv2d(512, 36, 1, 1, 0)\n",
        "        self.cls_head = nn.Conv2d(512, 9, 1, 1, 0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, feature_map):\n",
        "        output = self.relu(self.layer1(feature_map))\n",
        "        reg_out = self.relu(self.reg_head(output))\n",
        "        reg_out = self.flatten(reg_out)\n",
        "        reg_out = reg_out.view(-1,4)\n",
        "        cls_out = self.sigmoid(self.cls_head(output))\n",
        "        cls_out =  self.flatten(cls_out)\n",
        "        \n",
        "        return cls_out, reg_out"
      ],
      "id": "94baa7d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21b11f0e"
      },
      "outputs": [],
      "source": [
        "# First let us convert the array to tensor\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "anchor_locations = torch.tensor(anchor_locations, dtype = torch.float32)\n",
        "anchor_labels = torch.tensor(anchor_labels, dtype = torch.float32)\n",
        "\n",
        "\n",
        "raccoon_images = torch.tensor(raccoon_images, dtype = torch.float)\n"
      ],
      "id": "21b11f0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "493e73ef"
      },
      "outputs": [],
      "source": [
        "# Let us fix the hyperparameters for the model\n",
        "\n",
        "epochs = 10\n",
        "lr = 0.00001\n",
        "model = rpn()\n",
        "optim = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.9)"
      ],
      "id": "493e73ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF1kSrQZeRgB"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else :\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "fe_extractor.to(device)\n",
        "model.to(device)"
      ],
      "id": "QF1kSrQZeRgB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZUQAFojeZ4P"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range(len(raccoon_images)):\n",
        "        raccoon_image = torch.unsqueeze(raccoon_images[i], dim = 0)\n",
        "        offsets = anchor_locations[i]\n",
        "        classes = anchor_labels[i]\n",
        "        raccoon_image = torch.permute(raccoon_image, (0,3,1,2))\n",
        "        raccoon_image,offsets,classes = raccoon_image.to(device),offsets.to(device),classes.to(device)\n",
        "        features = fe_extractor(raccoon_image)\n",
        "        optim.zero_grad()\n",
        "        cls_output, reg_output = model(features)\n",
        "        cls_loss = binary_loss(cls_output[0], classes)\n",
        "        reg_loss = custom_l1_loss(reg_output, offsets, classes)\n",
        "        total_loss = cls_loss + reg_loss\n",
        "        total_loss.backward()\n",
        "        optim.step()\n",
        "        print(\"For epoch number {} and image number {}\".format(epoch+1,i+1))\n",
        "        print(\"The classification loss {}, the smooth l1 loss {}\".format(cls_loss, reg_loss))"
      ],
      "id": "BZUQAFojeZ4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRGNxkYTqYTM"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"weights.pth\")"
      ],
      "id": "yRGNxkYTqYTM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxHYSl1yc6Kg"
      },
      "outputs": [],
      "source": [
        "class classifier(nn.Module):\n",
        "   \n",
        "   def __init__(self):\n",
        "     super(classifier, self).__init__()\n",
        "     self.layer1 = nn.Flatten()\n",
        "     self.relu = nn.ReLU()\n",
        "     self.layer2 = nn.Linear(320000, 500)\n",
        "     self.layer3 = nn.Linear(500, 25)\n",
        "     self.layer4 = nn.Linear(25, 4)\n",
        "     self.layer5 = nn.Linear(4,1)\n",
        "     self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "   def forward(self,x):\n",
        "     x1 = self.layer1(x)\n",
        "     x2 = self.relu(self.layer2(x1))\n",
        "     x3 = self.relu(self.layer3(x2))\n",
        "     x4 = self.relu(self.layer4(x3))\n",
        "     x5 = self.sigmoid(self.layer5(x4))\n",
        "     \n",
        "     return x5"
      ],
      "id": "PxHYSl1yc6Kg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvGTc1OWbwiB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class custom_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, images, gt_boxes):\n",
        "    self.images = images\n",
        "    self.gt_boxes = gt_boxes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx],self.gt_boxes[idx]"
      ],
      "id": "vvGTc1OWbwiB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8whwkYpjqDR"
      },
      "outputs": [],
      "source": [
        "train_data = custom_dataset(raccoon_images, gt_boxes = gt_boxes)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = 1, shuffle = True)\n",
        "\n",
        "from torchvision.ops import roi_pool"
      ],
      "id": "d8whwkYpjqDR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7rBuUwAgp_g"
      },
      "outputs": [],
      "source": [
        "def output_processing(score, reg):\n",
        "  dy = reg[:,0].detach().numpy()\n",
        "  dx = reg[:,1].detach().numpy()\n",
        "  dh = reg[:,2].detach().numpy()\n",
        "  dw = reg[:,3].detach().numpy()\n",
        "\n",
        "  height = (anchor_boxes[:,:,2] - anchor_boxes[:,:,0]).flatten()\n",
        "  width = (anchor_boxes[:,:,3] - anchor_boxes[:,:,1]).flatten()\n",
        "  ctr_y = anchor_boxes[:,:,2].flatten() - height/2\n",
        "  ctr_x = anchor_boxes[:,:,3].flatten() - width/2\n",
        "\n",
        "  pred_ctr_y = dy*height + ctr_y\n",
        "  pred_ctr_x = dx*width + ctr_x\n",
        "  pred_height = np.exp(dh)*height\n",
        "  pred_width = np.exp(dw)*width\n",
        "\n",
        "  roi = np.zeros_like(reg.detach())\n",
        "  roi[:,0] = ctr_y - pred_height/2\n",
        "  roi[:,1] = ctr_x - pred_width/2\n",
        "  roi[:,2] = ctr_y + pred_height/2 \n",
        "  roi[:,3] = ctr_x + pred_width/2\n",
        "\n",
        "# It's time to clip the output so that the predictions lie within the size of the image\n",
        "\n",
        "  min_size = 16\n",
        "  roi = np.clip(roi,0,800) \n",
        "  hs = roi[:, 2] - roi[:, 0] \n",
        "  ws = roi[:, 3] - roi[:, 1]\n",
        "  keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
        "  roi = roi[keep, :]\n",
        "  score = score.detach().numpy()[0][keep]\n",
        "\n",
        "  return roi, score"
      ],
      "id": "x7rBuUwAgp_g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhynK3Li-dkp"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import nms\n",
        "\n",
        "def apply_nms(roi, score):\n",
        "  box_indices = nms(torch.tensor(roi), torch.tensor(score),iou_threshold = 0.7)\n",
        "  filtered_boxes = torch.tensor(roi[box_indices])\n",
        "  filtered_boxes = torch.cat((torch.tensor([[0]]*len(filtered_boxes)),filtered_boxes), dim = 1)\n",
        "  filtered_scores = torch.tensor(score[box_indices])\n",
        "\n",
        "  return filtered_boxes, filtered_scores"
      ],
      "id": "hhynK3Li-dkp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJt4GSNk3lRf"
      },
      "outputs": [],
      "source": [
        "model = torch.load('weights.pth').to(device).eval()\n",
        "fe_extractor.to(device)"
      ],
      "id": "iJt4GSNk3lRf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTaD28SStLhZ"
      },
      "outputs": [],
      "source": [
        "classify = classifier()\n",
        "loss = nn.BCELoss()\n",
        "classify.to(device)\n",
        "lr = 10**(-5)\n",
        "optim = torch.optim.SGD(classify.parameters(), lr = lr, momentum = 0.9)"
      ],
      "id": "kTaD28SStLhZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzeh2FuC6S6y"
      },
      "outputs": [],
      "source": [
        "def check_label(gt, rois):\n",
        "  \n",
        "  pos_threshold = 0.7\n",
        "  \n",
        "  labels = torch.empty(len(rois))\n",
        "  for cnt1 in range(len(rois)):\n",
        "    ROI = rois[cnt1]\n",
        "    max_iou = 0\n",
        "    gt_values = gt[0]\n",
        "    for cnt2 in range(len(gt_values)):\n",
        "      gt_box = gt_values[cnt2]\n",
        "      IOU = iou(gt_box, ROI)\n",
        "\n",
        "      if IOU > max_iou :\n",
        "        max_iou = IOU\n",
        "    \n",
        "    if max_iou >= pos_threshold :\n",
        "      labels[cnt1] = 1\n",
        "\n",
        "    else :\n",
        "       labels[cnt1] = 0\n",
        "\n",
        "  return labels  "
      ],
      "id": "Hzeh2FuC6S6y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG4eTGAD-ZCU"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs) :\n",
        "  for id, (raccoon_image,gt_values) in enumerate(train_loader):\n",
        "   with torch.no_grad(): \n",
        "    raccoon_image = torch.permute(raccoon_image,(0,3,1,2))\n",
        "    raccoon_image,gt_values = raccoon_image.to(device),gt_values.to(device)\n",
        "    map = fe_extractor(raccoon_image)\n",
        "    confidence, offsets = model(map)\n",
        "    final_offsets, confidence = output_processing(confidence.cpu(), offsets.cpu())\n",
        "    final_offsets,confidence = apply_nms(final_offsets, confidence)\n",
        "    final_offsets = final_offsets.to(device)\n",
        "    rois = roi_pool(map, final_offsets[:1000], output_size = 25, spatial_scale = 0.0625)\n",
        "    la = check_label(gt_values, final_offsets[:1000,1:])\n",
        "    optim.zero_grad()\n",
        "   output = classify(rois)\n",
        "   la = torch.unsqueeze(la, dim = 1)\n",
        "   la = la.to(device)\n",
        "   training_loss = loss(output, la)\n",
        "   training_loss.backward()\n",
        "   optim.step()\n",
        "   print(\"The Binary Cross Entropy loss for {} epoch {} image is\".format(epoch+1, id+1),training_loss)"
      ],
      "id": "fG4eTGAD-ZCU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFLWlfbaeMZ"
      },
      "outputs": [],
      "source": [
        "torch.save(classify, 'weights2.pth')"
      ],
      "id": "_aFLWlfbaeMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHaAKulMaRYk"
      },
      "outputs": [],
      "source": [
        "test_img = cv2.resize(cv2.imread(\"test.jpg\"), (800,800))\n",
        "clone = test_img.copy()"
      ],
      "id": "mHaAKulMaRYk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZO8ZHPtgK8E"
      },
      "outputs": [],
      "source": [
        "# Now that training is completed, we will work on test image\n",
        "\n",
        "test_img = torch.tensor(test_img, dtype = torch.float32)\n",
        "test_img = torch.permute(torch.unsqueeze(test_img, dim = 0), (0,3,1,2))\n",
        "\n",
        "fe_extractor.to(torch.device('cpu'))\n",
        "model.to(torch.device('cpu'))\n",
        "classify = torch.load(os.path.join(path,'weights2.pth')).to(torch.device('cpu')).eval()\n",
        "fe_map = fe_extractor(test_img)\n",
        "score, reg = model(fe_map)\n",
        "\n",
        "reg, score = output_processing(score, reg)\n",
        "reg, score = apply_nms(reg, score)\n",
        "roi = roi_pool(fe_map, reg[:100], output_size = 25, spatial_scale = 0.0625)\n",
        "output = classify(roi)"
      ],
      "id": "_ZO8ZHPtgK8E"
    },
    {
      "cell_type": "code",
      "source": [
        "score > 0.999"
      ],
      "metadata": {
        "id": "CnDU2y8pMgiv"
      },
      "id": "CnDU2y8pMgiv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XnaJVBnaaRd"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for i in range(len(score)):\n",
        "  if score[i] >= 0.999 :\n",
        "    cv2.rectangle(clone, (int(reg[i,1]),int(reg[i,2])), (int(reg[i,3]),int(reg[i,4])), (0,255,0), 2)\n",
        "\n",
        "cv2_imshow(clone)"
      ],
      "id": "1XnaJVBnaaRd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buU75ctyI8ci",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for i in range(len(output)):\n",
        "  if output[i] == output.max():\n",
        "    cv2.rectangle(clone, (int(reg[i,1]),int(reg[i,2])), (int(reg[i,3]),int(reg[i,4])), (0,255,0), 2)\n",
        "\n",
        "cv2_imshow(clone)"
      ],
      "id": "buU75ctyI8ci"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwSgN5hlqW7G"
      },
      "id": "XwSgN5hlqW7G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Deep_Learning",
      "language": "python",
      "name": "deep_learning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}